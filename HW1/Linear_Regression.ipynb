{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTES** \n",
    "- Please complete the code between the two comments: `## START CODE HERE` and `## END CODE HERE`. \n",
    "- Be sure to run the codes in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "- [1 - Introduction](#1)\n",
    "  - [1.1 Linear Regression](#1.1)\n",
    "  - [1.2 Loss Functions](#1.2)\n",
    "  - [1.3 Gradient Descent](#1.3)\n",
    "- [2 - Univariate Linear Regression](#2)\n",
    "- [3 - Multiple Variable Linear Regression](#3)\n",
    "  - [3.1 Vectorization](#3.1)\n",
    "  - [3.2 Matrix X Containing Our Examples](#3.2)\n",
    "  - [3.3 Parameter Vector w, b](#3.3)\n",
    "  - [3.4 Implementation](#3.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - Introduction\n",
    "\n",
    "In this section, you'll see a brief introduction to fundamental concepts of linear regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1.1\"></a>\n",
    "### 1.1 - Linear Regression\n",
    "\n",
    "Linear regression tries to find a linear connection between an independent variable and one or more dependent variables(a.k.a target values). In a basic linear regression model with just one independent variable, the equation can be represented as:\n",
    "\n",
    "$$f_{w,b}(x) = wx + b$$\n",
    "\n",
    "In this equation:\n",
    "- $y$: The dependent variable, which is the target, the model is trying to predict\n",
    "- $x$: The independent variable\n",
    "- $w$: the coefficient or weights which signifies the influence of an independent variable on the dependent variable \n",
    "- $b$: The intercept or bias  \n",
    "\n",
    "$f_{w,b}(x)$ can also be shown as $\\hat{y}$. They are the same.  \n",
    "The fundamental objective of linear regression is to determine the values of w and b that minimize the difference between the predicted values and the actual data points within the dataset. This minimization process is usually achieved by employing a cost function, such as the mean squared error (MSE), and optimization techniques like gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple datasat as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 3, 5, 7])\n",
    "y = np.array([2000, 3400, 4800, 6200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, c=\"r\", marker=\"x\", label=\"Actual Values\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\", rotation=0)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test different values of 'w' and 'b' for yourself. Let's see if you can discover the linear connection between 'x' and 'y'!  \n",
    "P.S.: `f_wb` stands for 'f(w,b)', which denotes a function of 'w' and 'b'. `f_wb` and `y_pred` are interchangeably used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## START CODE HERE\n",
    "w = \n",
    "b = \n",
    "## END CODE HERE\n",
    "f_wb = w*x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual data points\n",
    "plt.scatter(x, y, c=\"r\", marker=\"x\", label=\"Actual Values\")\n",
    "\n",
    "# plot your prediction\n",
    "plt.plot(x, f_wb, c=\"b\", label=\"Your Prediction\" )\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\", rotation=0)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1.2\"></a>\n",
    "### 1.2 - Loss Functions\n",
    "\n",
    "Loss functions calculate the difference between real and predicted values. .These functions provide a measure of how well the model is performing in terms of its ability to make accurate predictions. Two commonly used loss functions in linear regression are the Mean Squared Error (MSE) and the Mean Absolute Error (MAE).\n",
    "\n",
    "#### Mean Squared Error (MSE)\n",
    "\n",
    "It is perhaps the most widely used loss function in linear regression. The equation for MSE is as follows:\n",
    "\n",
    "$$J(w,b) = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2$$ \n",
    "Where:\n",
    "- $m$ is the number of data points.\n",
    "- $y_i$ represents the actual target value.\n",
    "- $f_{w,b}(x^{(i)})$ represents the predicted value.\n",
    "\n",
    "#### Mean Absolute Error (MAE)\n",
    "\n",
    "The Mean Absolute Error provides an alternative approach to measuring prediction errors. The equation for MAE is as follows:\n",
    "\n",
    "$$J(w,b) = \\frac{1}{m} \\sum\\limits_{i=0}^{m-1} |f_{w,b}(x^{(i)}) - y^{(i)}|$$\n",
    "\n",
    "Where:\n",
    "- $m$ is the number of data points.\n",
    "- $y_i$ represents the actual target value.\n",
    "- $f_{w,b}(x^{(i)})$ represents the predicted value.\n",
    "\n",
    "#### Difference between Loss and Cost Functions\n",
    "\n",
    "While the terms \"loss function\" and \"cost function\" are sometimes used interchangeably, there is a subtle difference between the two. The loss function calculates the error for a single data point, while the cost function (or objective function) measures the overall error for the entire dataset. In the context of linear regression, the MSE and MAE are used as loss functions, and they can be summed up to create the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have a couple of options to calculate costs:\n",
    "\n",
    "You can use `np.sum()` to sum all the squared or absolute values (losses) and then perform the division.  \n",
    "Alternatively, you can use `np.mean()` to combine the summation and division parts.  \n",
    "Another approach is to use a simple for loop to calculate the loss function, and by adding those individual losses, you can obtain the cost value.  \n",
    "\n",
    "Also To square each element in a NumPy array, you can use the `**` operator or the `np.square()` function.\n",
    "\n",
    "Try implementing these two cost functions. You can use `np.sum()`, `np.mean()` or a simple loop over elements in the y_true and y_pred elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "## START CODE HERE\n",
    "\n",
    "## END CODE HERE\n",
    "print(f'Mean Squared Error (MSE) = {mean_squared_error(y, f_wb)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(y_true, y_pred):\n",
    "## START CODE HERE\n",
    "\n",
    "## END CODE HERE\n",
    "print(f'Mean Absolute Error (MAE) = {mean_absolute_error(y, f_wb)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check the results with a calculator to ensure your gwt the correct results! Incorrect mathematical operations can lead to incorrect outcomes, potentially affecting your assignment's grade!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1.3\"></a>\n",
    "### 1.3 - Gradient Descent\n",
    "\n",
    "Gradient Descent is a fundamental optimization algorithm used in machine learning and data science to iteratively improve the performance of models. It's a process of fine-tuning model parameters to minimize a specified cost function.\n",
    "\n",
    "The \"gradient\" in gradient descent refers to the steepness of the slope or the direction of the steepest increase in the cost function. By repeatedly calculating the gradient and updating model parameters, gradient descent progressively refines the model's ability to make accurate predictions. This iterative process continues until the cost function reaches a minimum, indicating that the model has achieved the best possible fit to the data.\n",
    "\n",
    "The gradient descent algorithm is:\n",
    "\n",
    "$$\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & \\phantom {0000} w = w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\; & \\newline \\; & \\phantom {0000} b = b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b} \\newline & \\rbrace\\end{align*}$$\n",
    "\n",
    "And when the loss function is MSE, $\\frac{\\partial J(w,b)}{\\partial w}$ and $\\frac{\\partial J(w,b)}{\\partial w}$ are calculated as follows:\n",
    "$$\\frac{\\partial J(w,b)}{\\partial w}  = \\frac{2}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) -y^{(i)})x^{(i)}$$\n",
    "$$\\frac{\\partial J(w,b)}{\\partial b}  = \\frac{2}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})$$\n",
    "\n",
    "* m is the number of training examples in the dataset\n",
    "* $\\alpha$ is learning rate \n",
    "*  $f_{w,b}(x^{(i)})$ is the model's prediction, while $y^{(i)}$, is the target value\n",
    "\n",
    "Perform the mathematical derivations yourself to gain a better understanding of these equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 200\n",
    "b = 1500\n",
    "f_wb = w*x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual data points\n",
    "plt.scatter(x, y, c=\"r\", marker=\"x\", label=\"Actual Values\")\n",
    "\n",
    "# plot initial prediction with initial weights and bias\n",
    "plt.plot(x, f_wb, c=\"b\", label=\"Prediction With Initial Weights and Bias\" )\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\", rotation=0)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we want to use MSE loss function for this regression task, then calculate $\\frac{\\partial J(w,b)}{\\partial w}$ and $\\frac{\\partial J(w,b)}{\\partial b}$ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  START CODE HERE\n",
    "dJ_dw = \n",
    "dJ_db = \n",
    "## END CODE HERE\n",
    "print(f'dJ/dw:{dJ_dw}\\ndJ/db:{dJ_db}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><b>dJ/dw:</td>\n",
    "    <td>-19400.0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>dJ/db:</td>\n",
    "    <td>-3600.0</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_prev = w\n",
    "b_prev = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the update step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## START CODE HERE\n",
    "w = \n",
    "b = \n",
    "## END CODE HERE\n",
    "f_wb = w*x + b\n",
    "print(f'Updated values of weights and bias. w = {w}, b = {b}')\n",
    "print(f'Mean Absolute Error (MAE) after 1 iteration of Gradient Descent = {mean_squared_error(y, f_wb)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>Updated values of weights and bias. w = 588.0, b = 1572.0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Mean Absolute Error (MAE) after 1 iteration of Gradient Descent = 93696.0</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual data points\n",
    "plt.scatter(x, y, c=\"r\", marker=\"x\", label=\"Actual Values\")\n",
    "\n",
    "# plot initial prediction with initial weights and bias\n",
    "plt.plot(x, f_wb, c=\"b\", label=\"Prediction After 1 Iteration of Gradient Descent\" )\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\", rotation=0)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_values = np.arange(0, 1500)\n",
    "predictions = np.array([w*x for w in w_values])\n",
    "J_values = np.array([mean_squared_error(y, prediction) for prediction in predictions])\n",
    "plt.plot(w_values, J_values, c='b')\n",
    "plt.xlabel('w')\n",
    "plt.ylabel('J(w)', rotation=0)\n",
    "plt.arrow(w_prev, mean_squared_error(y, w_prev*x), w-w_prev,  mean_squared_error(y, w*x)-mean_squared_error(y, w_prev*x), ec='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see in the figure of `J(w)` vs. `w`, that after one iteration of gradient descent, cost function got closser to the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 - Univariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the following cell to load and then use a more realistic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('salary_dataset.csv')\n",
    "x = np.array(df.iloc[:, 0])\n",
    "y = np.array(df.iloc[:, 1])\n",
    "print ('Number of training examples (m):', x.shape[0])\n",
    "df.head().iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, marker='x', c='m') \n",
    "plt.title(\"Year of Experience vs. Salary\")\n",
    "plt.ylabel('salary')\n",
    "plt.xlabel('years of experience')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, complete the code with codes you previously wrote. Feel free to use `np.sum` or `np.mean`. Using these built-in NumPy functions is recommended over manual and verbose loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train_gradient_descent(self, x, y, learning_rate=0.01, n_iters=100):\n",
    "        \"\"\"\n",
    "        Trains a univariate linear regression model using gradient descent\n",
    "        \"\"\"\n",
    "        # Step 0: Initialize parameters\n",
    "        n_samples = x.shape[0]\n",
    "        self.w = 0\n",
    "        self.b = 0\n",
    "        costs = []\n",
    "\n",
    "        for i in range(n_iters):\n",
    "            # Step 1: Compute a linear combination of input and weight\n",
    "            ## START CODE HERE\n",
    "            y_predict = \n",
    "            # END CODE HERE\n",
    "\n",
    "            # Step 2: Compute cost over training set\n",
    "            ## START CODE HERE\n",
    "            cost = \n",
    "            ## END CODE HERE\n",
    "            costs.append(cost)\n",
    "\n",
    "            print(f\"Cost at iteration {i}: {cost}\")\n",
    "\n",
    "            # Step 3: Compute gradients\n",
    "            ## START CODE HERE\n",
    "            dJ_dw = \n",
    "            dJ_db = \n",
    "            # END CODE HERE\n",
    "            \n",
    "            # Step 4: Update weight and bias\n",
    "            ## START CODE HERE\n",
    "            self.w = \n",
    "            self.b = \n",
    "            # END CODE HERE\n",
    "\n",
    "        return self.w, self.b, costs\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.w * x + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression()\n",
    "w_trained, b_trained, costs = regressor.train_gradient_descent(x, y, learning_rate=0.001, n_iters=20)\n",
    "\n",
    "plt.plot(np.arange(len(costs)), costs)\n",
    "plt.title(\"Cost during training\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yrs_of_expr = 7.5\n",
    "prediction = regressor.predict(yrs_of_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, marker='x', c='m') \n",
    "plt.plot(x, regressor.predict(x), label='predicted line', c='b')\n",
    "plt.plot(yrs_of_expr, prediction, label=f'predicted salary for {yrs_of_expr}', marker='o', c='r', markersize=6)\n",
    "plt.title(\"Year of Experience vs. Salary\")\n",
    "plt.ylabel('salary')\n",
    "plt.xlabel('years of experience')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## 3 - Multiple Variable Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multiple variable linear regression, we expand our ability to predict outcomes using not just one, but several independent variables.\n",
    "First, let's familiarize ourselves with some key concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.1\"></a>\n",
    "### 3.1 - Vectorization\n",
    "Vectors vs For Loops, which one to use? We prefer vectorization because of  its brevity and superior efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using a for loop** implement a function which gets two vectors and output their dot product:(don't worry about the potential shape mismatch).\n",
    "$$ a.b = \\sum_{i=0}^{n-1} a_i b_i $$\n",
    "Assume both `a` and `b` are the same shape, (n,)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(a , b):\n",
    "    ## START CODE HERE\n",
    "    \n",
    "    ## END CODE HERE\n",
    "a = np.array([1, 2, 3, 4, 5])\n",
    "b = np.array([1, 1, 0, 1, 0])\n",
    "print(f'a.b={dot_product(a, b)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>a.b=7</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do a speed comparison between dot product calculation you've implemented using a for loop and numpy's built-in version which use vectorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "a = np.random.rand(10000000)\n",
    "b = np.random.rand(10000000)\n",
    "\n",
    "tic = time.time()\n",
    "c = np.dot(a, b)\n",
    "toc = time.time()\n",
    "print(f\"np.dot(a, b) =  {c:.4f}\")\n",
    "print(f\"Vectorized version duration: {1000*(toc-tic):.4f} ms \")\n",
    "\n",
    "tic = time.time()\n",
    "c = dot_product(a,b)\n",
    "toc = time.time()\n",
    "print(f\"my_dot(a, b) =  {c:.4f}\")\n",
    "print(f\"loop version duration: {1000*(toc-tic):.4f} ms \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, vectorization provides a large speed up in this example. This is because NumPy makes better use of available data parallelism in the underlying hardware. GPU's and modern CPU's implement Single Instruction, Multiple Data (SIMD) pipelines allowing multiple operations to be issued in parallel. This is critical in Machine Learning where the data sets are often very large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.2\"></a>\n",
    "### 3.2 - Matrix X Containing Our Examples\n",
    "\n",
    "Consider the following task illustrated in the table below. The rows represent training samples, while the columns are input features.\n",
    "\n",
    "| Size (sqft) | Number of Bedrooms  | Number of Floors | Age of Home | Price (1000s dollars)  |\n",
    "| ---------------- | ------------------- | ---------------- | ----------- | ----------- |\n",
    "| 2104                | 5                           | 1                       | 45                | 460                |\n",
    "| 1416                | 3                           | 2                       | 40                | 232                |\n",
    "| 852                  | 2                           | 1                       | 35                | 178                |\n",
    "\n",
    "In multiple variable linear regression, it's common to organize our inputs in a matrix and our weights and outputs in vectors. We typically use `X` to denote our input matrix. Each row of the matrix represents one example. In a scenario with $m$ training examples (in this example, $m$ is three), and there are $n$ features (four in this example), the matrix $\\mathbf{X}$ has dimensions ($m$, $n$) â€“ m rows and n columns.\n",
    "\n",
    "$$\\mathbf{X} = \n",
    "\\begin{pmatrix}\n",
    " x^{(0)}_0 & x^{(0)}_1 & \\cdots & x^{(0)}_{n-1} \\\\ \n",
    " x^{(1)}_0 & x^{(1)}_1 & \\cdots & x^{(1)}_{n-1} \\\\\n",
    " \\cdots \\\\\n",
    " x^{(m-1)}_0 & x^{(m-1)}_1 & \\cdots & x^{(m-1)}_{n-1} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Notation:\n",
    "- $\\mathbf{x}^{(i)}$ is a vector containing example i: $\\mathbf{x}^{(i)} = (x^{(i)}_0, x^{(i)}_1, \\cdots,x^{(i)}_{n-1})$\n",
    "- $x^{(i)}_j$ is element j in example i. The superscript in parentheses indicates the example number, while the subscript represents an element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.3\"></a>\n",
    "### 3.3 Parameter Vector w, b\n",
    "\n",
    "* $\\mathbf{w}$ is a vector with $n$ elements.\n",
    "  - Each element contains the parameter associated with one feature.\n",
    "  - notionally, we draw this as a column vector\n",
    "\n",
    "$$\\mathbf{w} = \\begin{pmatrix}\n",
    "w_0 \\\\ \n",
    "w_1 \\\\\n",
    "\\cdots\\\\\n",
    "w_{n-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "* $b$ is a scalar parameter.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.4\"></a>\n",
    "### 3.4 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Prediction\n",
    "The model's prediction with multiple variables is given by the linear model:\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x^{(i)}}) =  w_0x^{(i)}_0 + w_1x^{(i)}_1 +... + w_{n-1}x^{(i)}_{n-1} + b$$\n",
    "or in vector notation:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x^{(i)}}) = \\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b $$ \n",
    "where $\\cdot$ is a vector `dot product` and $\\mathbf{x^{(i)}}$ is **i-th sample** of the input, with shape (num_features,).  \n",
    "\n",
    "We can extend this to calculate $f_{\\mathbf{w},b}$ the for all samples.\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{X}) = \\mathbf{w} \\cdot \\mathbf{X} + b $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following code to calculate single prediction from the equation below (vector notation). Use `np.dot`.  \n",
    "`X` is of shape $(m, n)$ and `w` is of shape $(n,)$, and the result of dot product will be of shape $(m,)$ . Adding this to a scallar(`b`) yeilds us a shape of $(m,)$ for `f_wb`. Look up broadcasting in python for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "num_samples = 20\n",
    "num_features = 5\n",
    "X = np.random.randint(1, 10, size=(num_samples, num_features)) # shape = (20, 5)\n",
    "y = np.random.randint(1, 10, size=(num_samples)) # shape =(20,)\n",
    "w = np.random.randint(1, 10, size=num_features) # shape = (5,)\n",
    "b = np.random.randint(1, 10) # scalar\n",
    "\n",
    "m = X.shape[0]\n",
    "## START CODE HERE               \n",
    "f_wb_X = \n",
    "## END CODE HERE\n",
    "print(f'y_true:{y}\\ny_pred:{f_wb_X}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><b>y_true:</td>\n",
    "    <td>[3 8 3 7 1 3 7 7 3 8 8 1 7 6 2 5 7 1 7 6]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>y_pred:</td>\n",
    "    <td>[ 63  86  75  98  70 119  89  79  52 136 109  69  86  81  97 116 132  66  82  72]</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Cost\n",
    "The equation for the MSE cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2$$ \n",
    "where:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$ \n",
    "\n",
    "\n",
    "In contrast to univariate linear regression, $\\mathbf{w}$ and $\\mathbf{x}^{(i)}$ are vectors rather than scalars.\n",
    "\n",
    "Complete the following code to calculate cost over all training samples.  \n",
    "**HINT**: You can use `np.sum` to sum over elemnts of $f_{\\mathbf{w},b}(\\mathbf{X})$ minus $y$, then devide by `m`, both $f_{\\mathbf{w},b}(\\mathbf{X})$ and $y$ are of shape $(m,)$(refer to the previous cell to see why $f_{\\mathbf{w},b}(\\mathbf{X})$  has this shape). Use `**2` to square elements of an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "num_samples = 20\n",
    "num_features = 5\n",
    "X = np.random.randint(1, 10, size=(num_samples, num_features)) # shape = (20, 5)\n",
    "y = np.random.randint(1, 10, size=(num_samples)) # shape =(20,)\n",
    "w = np.random.randint(1, 10, size=num_features) # shape = (5,)\n",
    "b = np.random.randint(1, 10) # scalar\n",
    "\n",
    "m = X.shape[0]\n",
    "## START CODE HERE               \n",
    "f_wb_X =  # same implementation as the previous cell\n",
    "cost = \n",
    "## END CODE HERE\n",
    "print(f'y_true:{y}\\ny_pred:{f_wb_X}')\n",
    "print(f'cost:{cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected Output:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><b>y_true:</td>\n",
    "    <td>[3 8 3 7 1 3 7 7 3 8 8 1 7 6 2 5 7 1 7 6]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>y_pred:</td>\n",
    "    <td>[ 63  86  75  98  70 119  89  79  52 136 109  69  86  81  97 116 132  66  82  72]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>cost:</td>\n",
    "    <td>7496.35</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><b><font size=\"5\">**********************OPTIONAL**********************</font></b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent for Multiple Variables\n",
    "\n",
    "Compute the partial derivatives of the cost function with respect to each parameter:\n",
    "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{2}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}$$\n",
    "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{2}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})$$\n",
    "* m is the number of training examples in the dataset\n",
    "\n",
    "The gradient containing all partial derivatives can then be computed as follows:(I wrote $\\hat{y}$ instead of $f_{\\mathbf{w},b}(\\mathbf{X})$, for simplicity)\n",
    "\n",
    "$$\\nabla_{\\boldsymbol{w}} J = \\frac{2}{m} \\boldsymbol{X}^T \\cdot \\big(\\boldsymbol{\\hat{y}} - \\boldsymbol{y} \\big)$$\n",
    "$$\\nabla_{\\boldsymbol{b}} J = \\frac{2}{m} \\big(\\boldsymbol{\\hat{y}} - \\boldsymbol{y} \\big)$$\n",
    "\n",
    "Pay attention to the fact that both $\\hat{y}$ and $y$ are of shape $(m,)$ and $X$ is of shape $(m, n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You don't need to implement anything *NEW* in the following cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "num_samples = 20\n",
    "num_features = 5\n",
    "X = np.random.randint(1, 10, size=(num_samples, num_features)) # shape = (20, 5)\n",
    "y = np.random.randint(1, 10, size=(num_samples)) # shape =(20,)\n",
    "w = np.random.randint(1, 10, size=num_features) # shape = (5,)\n",
    "b = np.random.randint(1, 10) # scalar\n",
    "\n",
    "m = X.shape[0]\n",
    "## START CODE HERE               \n",
    "f_wb_X =  # same implementation as the two previous cell\n",
    "cost =   # same implementation as the previous cell\n",
    "## END CODE HERE\n",
    "dJ_dw = (2 / m) * np.dot(X.T, (f_wb_X - y)) # same shape as w which is (5,)\n",
    "dJ_db = (2 / m) * np.sum((f_wb_X - y)) # same shape as b which is scalar\n",
    "\n",
    "print(f'y_true:{y}\\ny_pred:{f_wb_X}')\n",
    "print(f'dJ_dw:{dJ_dw}\\ndJ_db:{dJ_db}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected Output:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><b>y_true:</td>\n",
    "    <td>[3 8 3 7 1 3 7 7 3 8 8 1 7 6 2 5 7 1 7 6]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>y_pred:</td>\n",
    "    <td>[ 63  86  75  98  70 119  89  79  52 136 109  69  86  81  97 116 132  66  82  72]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>dJ_dw:</td>\n",
    "    <td>[ 852.9 1088.4 1090.1  775.1  780.8]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>dJ_db:</td>\n",
    "    <td>167.70000000000002</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionMultipleVariable:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train_gradient_descent(self, X, y, learning_rate=0.01, n_iters=100):\n",
    "        \"\"\"\n",
    "        Trains a univariate linear regression model using gradient descent\n",
    "        \"\"\"\n",
    "        # Step 0: Initialize parameters\n",
    "        n_samples, n_features = X.shape\n",
    "        self.w = np.zeros(shape=(n_features,))\n",
    "        self.b = 0\n",
    "        costs = []\n",
    "\n",
    "        for i in range(n_iters):\n",
    "            # Step 1: Compute a linear combination of input and weight\n",
    "            ## START CODE HERE\n",
    "            y_predict = \n",
    "            # END CODE HERE\n",
    "\n",
    "            # Step 2: Compute cost over training set\n",
    "            ## START CODE HERE\n",
    "            cost = \n",
    "            ## END CODE HERE\n",
    "            costs.append(cost)\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Cost at iteration {i}: {cost}\")\n",
    "\n",
    "            # Step 3: Compute gradients\n",
    "            ## START CODE HERE\n",
    "            dJ_dw = \n",
    "            dJ_db = \n",
    "            # END CODE HERE\n",
    "            \n",
    "            # Step 4: Update weight and bias\n",
    "            ## START CODE HERE\n",
    "            self.w = \n",
    "            self.b = \n",
    "            # END CODE HERE\n",
    "\n",
    "        return self.w, self.b, costs\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_samples=1000, n_features=6, noise=1, random_state=42)\n",
    "X_train = X[:800]\n",
    "y_train = y[:800]\n",
    "X_test = X[800:]\n",
    "y_test = y[800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_subplots = X_train.shape[1]\n",
    "fig, axes = plt.subplots(1, num_subplots, figsize=(20, 4))\n",
    "for i in range(num_subplots):\n",
    "    axes[i].scatter(X_train[:, i], y_train, label=f'Feature {i+1}', color=f'C{i}', s=10)\n",
    "    axes[i].set_xlabel(f'Feature {i+1}')\n",
    "    axes[i].set_ylabel('y')\n",
    "    axes[i].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegressionMultipleVariable()\n",
    "w_trained, b_trained, costs = regressor.train_gradient_descent(X_train, y_train, learning_rate=0.01, n_iters=200)\n",
    "\n",
    "plt.plot(np.arange(len(costs)), costs)\n",
    "plt.title(\"Cost during training\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_subplots = X_train.shape[1]\n",
    "fig, axes = plt.subplots(1, num_subplots, figsize=(20, 4))\n",
    "for i in range(num_subplots):\n",
    "    axes[i].scatter(X_test[:, i], y_test, label=f'true_label', marker='x', color=f'C{i}', s=20)\n",
    "    axes[i].scatter(X_test[:, i], regressor.predict(X_test), label=f'predicted label', marker='o', color=f'C{i+1%6}', s=10)\n",
    "    axes[i].set_xlabel(f'Feature {i+1}')\n",
    "    axes[i].set_ylabel('y')\n",
    "    axes[i].legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
