{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTES** \n",
    "- Please complete the code between the two comments: `## START CODE HERE` and `## END CODE HERE`. \n",
    "- Be sure to run the codes in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "seed= 1\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "- [1 -Binary Logistic Regression](#1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=500, centers= [(0, 5), (5, 0)], cluster_std=3, random_state=seed)\n",
    "y = y.reshape(-1, 1)\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.scatter(X[:,0], X[:,1], c=y)\n",
    "plt.title(\"Dataset\")\n",
    "plt.xlabel(\"First feature\")\n",
    "plt.ylabel(\"Second feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the data, using 80% for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## START CODE HERE\n",
    "\n",
    "## END CODE HERE\n",
    "print(f'Shape X_train: {X_train.shape}')\n",
    "print(f'Shape y_train: {y_train.shape}')\n",
    "print(f'Shape X_test: {X_test.shape}')\n",
    "print(f'Shape y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a foundational method widely used for modeling the relationship between a binary or categorical dependent variable and one or more independent variables. It is particularly well-suited for classification tasks, where the objective is to predict a discrete outcome, often represented as class labels (e.g., 0 or 1).\n",
    "\n",
    "Here we'll focus on logistic regression for binary classification. In logistic regression, the model estimates the probability that an instance belongs to a particular class. This estimation is achieved by applying the logistic function, also known as the sigmoid function, to the linear combination of the independent variables ($w \\cdot x + b$). The logistic regression equation can be expressed as:\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(x) = \\boldsymbol{\\hat{y}} = \\sigma(\\mathbf{w}\\cdot \\mathbf{x} + b) $$\n",
    "where function $\\sigma$ is the Sigmoid function. The sigmoid function is defined as:\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "In this equation:\n",
    "- $x$: The independent variable\n",
    "- $w$: the coefficient or weights which signifies the influence of an independent variable on the dependent variable \n",
    "- $b$: The intercept or bias  \n",
    "\n",
    "The primary objective of logistic regression is to identify the optimal values of $w$ and $b$ that align best with the data. This optimization process typically involves maximizing the likelihood of the observed data or minimizing a chosen cost function, such as the log-likelihood or cross-entropy loss. Commonly employed optimization techniques include gradient descent.\n",
    "Cross-entropy cost funcntion is as follows:\n",
    "$$J(\\boldsymbol{w},b) = - \\frac{1}{m} \\sum_{i=1}^m [ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)})]$$\n",
    "We want to model the probability of the target values being 0 or 1. So during training we want to adapt our parameters such that our model outputs high values for examples with a positive label (true label being 1) and small values for examples with a negative label (true label being 0). This is reflected in this equation.\n",
    "\n",
    "Might be suprising but if you do the derivation from calculus, you'll find out that the formula for computing partial derivatives of the cost function with respect to each parameter is exqctly like what it was for linear regression:\n",
    "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (\\hat{y}^{(i)} - y^{(i)})x_{j}^{(i)}$$\n",
    "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (\\hat{y}^{(i)} - y^{(i)})$$\n",
    "* m is the number of training examples in the dataset\n",
    "\n",
    "Yeeeah! but No! Here, in logistic regression the definition of $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is different!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With help of formulas given above and your own knowladge, complete the following code. \n",
    "**You are encouraged  to use numpy's built-in functions like: `np.exp`, `np.sum`, `np.argmax` etc.\n",
    "You may not use scikit-learn's built-in functions to facilitate the following code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "class LogisticRegression:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        '''\n",
    "        Args:\n",
    "            z: numpy array of shape = (n_samples, 1)\n",
    "        Returns:\n",
    "            g: numpy array of shape = (n_samples, 1)\n",
    "        '''\n",
    "        ## START CODE HERE\n",
    "\n",
    "        ## END CODE HERE\n",
    "\n",
    "    def train(self, X, y, n_iters, lr):\n",
    "        '''        \n",
    "        Args:\n",
    "            X: numpy array of shape = (n_samples, n_features)\n",
    "            y: numpy array of shape = (n_samples, 1)\n",
    "            n_iters: number of iterations. scalar\n",
    "            lr: learning rate. scalar\n",
    "        Returns:\n",
    "            w: numpy array of shape = (n_features, 1)\n",
    "            b: bias. scalar\n",
    "            costs: list of cost for each iteration\n",
    "        '''\n",
    "        n_samples, n_features = X.shape\n",
    "        ## START CODE HERE\n",
    "        # Initialize weights and bias to zero values, and ensure you monitor their dimensions.\n",
    "        \n",
    "        \n",
    "        ## END CODE HERE\n",
    "        costs = []\n",
    "        \n",
    "        for i in range(n_iters):\n",
    "            ## START CODE HERE\n",
    "            # Step 1: Compute a linear combination of the input features and weights\n",
    "\n",
    "            # Step 2: Apply the Sigmoid activation function\n",
    "            \n",
    "            # Step 3: Compute the cost\n",
    "\n",
    "            # Step 4: Compute the gradients\n",
    "\n",
    "            # Step 5: Update the parameters\n",
    "\n",
    "            # Also print the cost each 10 iterations\n",
    "\n",
    "            ## END CODE HERE\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Args:\n",
    "            X: numpy array of shape = (n_samples, n_features)\n",
    "        Returns:\n",
    "            numpy array of shape = (n_samples, 1) with predicted classes.(assume threshhold is 0.5)\n",
    "        '''\n",
    "        ## START CODE HERE\n",
    "    \n",
    "        ## END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LogisticRegression()\n",
    "w_trained, b_trained, costs = regressor.train(X_train, y_train, n_iters=, lr=) # choose learning rate and number of iterations, you think is best.\n",
    "\n",
    "plt.plot(np.arange(len(costs)), costs)\n",
    "plt.title(\"Cost during training\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)\n",
    "df = pd.DataFrame({'y_test': y_test.squeeze(), 'y_pred': y_pred.squeeze()})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "These are some metrics for classification tasks provide insights into a model's ability to correctly classify instances into different classes:\n",
    "- true positive (TP): The model classifies the example as positive, and the actual label also positive.\n",
    "- false positive (FP): The model classifies the example as positive, but the actual label is negative.\n",
    "- true negative (TN): The model classifies the example as negative, and the actual label is also negative.\n",
    "- false negative (FN): The model classifies the example as negative, but the label is actually positive.  \n",
    "\n",
    "- **Accuracy**: Measures the proportion of correct predictions.\n",
    "  $$accuracy = \\frac{\\text{true positives} + \\text{true negatives}}{\\text{true positives} + \\text{true negatives} + \\text{false positives} + \\text{false negatives}}$$\n",
    "\n",
    "- **Precision**: Quantifies the accuracy of positive predictions.\n",
    "  $$precision = \\frac{\\text{true positives}}{\\text{true positives} + \\text{false positives}}$$\n",
    "\n",
    "- **Recall**: Evaluates the model's capability to capture all positive instances.\n",
    "  $$recall = \\frac{\\text{true positives}}{\\text{true positives} + \\text{false negatives}}$$\n",
    "\n",
    "- **F1 Score**: A composite metric that balances precision and recall.\n",
    "  $$F1 = \\frac{2 \\times \\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_binary_classification_eval_metrics(y_true, y_pred):\n",
    "    # START CODE HERE\n",
    "    \n",
    "    \n",
    "    # END CODE HERE\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"True Positives (tp): {tp}\")\n",
    "    print(f\"True Negatives (tn): {tn}\")\n",
    "    print(f\"False Positives (fp): {fp}\")\n",
    "    print(f\"False Negatives (fn): {fn}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "compute_binary_classification_eval_metrics(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
